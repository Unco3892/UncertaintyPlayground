{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Uncertainty Playground: Fast estimation of prediction intervals with neural networks","text":"<p>This Python library provides fast ( and easy) uncertainty estimation for regression tasks built on top of <code>PyTorch</code> &amp; <code>GPyTorch</code>. Specifically, the library uses <code>Sparse &amp; Variational Gaussian Process Regression</code> for Gaussian cases (normally distributed outcomes) and <code>Mixed Density Networks</code> for multi-modal cases. Users can estimate the prediction interval for a given instance with either model. The library was developed and maintained by Ilia Azizi. Please note that this version is still in early development and not ready for production use. </p>"},{"location":"#installation","title":"Installation","text":"<p>Requirements:</p> <ul> <li>Python &gt;= 3.8</li> <li>PyTorch == 2.0.1</li> <li>GPyTorch == 1.10</li> <li>Numpy == 1.24.0</li> <li>Seaborn == 0.12.2</li> </ul> <p>From the root directory of the repo, run the following in your terminal:</p> <pre><code>pip install .</code></pre> <p>Then, you can import the module:</p> <pre><code>import uncertaintyplayground as up</code></pre>"},{"location":"#package-structure","title":"Package structure","text":"<pre><code>UncertaintyPlayground/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 tox.ini # Local continuous integration\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 docs # Documentation\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bib.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 example.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gen_ref_pages.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 examples # Examples of the package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 compare_models_example.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mdn_example.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 svgp_example.py\n\u2514\u2500\u2500 uncertaintyplayground # Main package\n    \u251c\u2500\u2500 requirements.txt\n    \u251c\u2500\u2500 models # Models\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 mdn_model.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 svgp_model.py\n    \u251c\u2500\u2500 trainers # Training the models\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 base_trainer.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 mdn_trainer.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 svgp_trainer.py\n    \u251c\u2500\u2500 predplot # Single instance prediction (inference) plot\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 grid_predplot.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 mdn_predplot.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 svgp_predplot.py\n    \u251c\u2500\u2500 utils # Utility functions\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 early_stopping.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 generate_data.py\n    \u2514\u2500\u2500 tests # Unit tests\n        \u251c\u2500\u2500 test_early_stopping.py\n        \u251c\u2500\u2500 test_generate_data.py\n        \u251c\u2500\u2500 test_mdn_model.py\n        \u251c\u2500\u2500 test_mdn_predplot.py\n        \u251c\u2500\u2500 test_mdn_trainer.py\n        \u251c\u2500\u2500 test_svgp_model.py\n        \u251c\u2500\u2500 test_svgp_predplot.py\n        \u2514\u2500\u2500 test_svgp_trainer.py</code></pre>"},{"location":"#documentation-layout","title":"Documentation layout","text":"<p>Aside from this page, there are three other sections in this documentation. The most important is the <code>Code Reference</code> which relates the source code and all the arguments for the functions.  The <code>Usage</code> section contains a couple of examples of using this package with real and simulated data. Finally, the <code>Bibliography</code> section contains the list of papers that are used in this package.</p>"},{"location":"bib/","title":"Bibliography","text":""},{"location":"bib/#references","title":"References","text":"<ol> <li>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S.</li> <li>Gardner, J.R., Pleiss, G., Bindel, D., Weinberger, K.Q. and Wilson, A.G. (2018). GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. In Advances in Neural Information Processing Systems.</li> <li>Rasmussen, C.E. and Williams, C.K.I. (2005). Gaussian Processes for Machine Learning. The MIT Press. doi:10.7551/mitpress/3206.001.0001</li> <li>Bishop, C.M. (1994). Mixture Density Networks. Aston University.</li> <li>Titsias, M. (2009). Variational Learning of Inducing Variables in Sparse Gaussian Processes. In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, vol. 5, pp. 567--574. https://proceedings.mlr.press/v5/titsias09a.html</li> </ol>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#real-data","title":"Real data","text":"<p>This example uses the California Housing dataset from Sklearn. You can train and visualize the results of the models in the following way:</p> <pre><code>from uncertaintyplayground.trainers.svgp_trainer import SparseGPTrainer\nfrom uncertaintyplayground.trainers.mdn_trainer import MDNTrainer\nfrom uncertaintyplayground.predplot.svgp_predplot import compare_distributions_svgpr\nfrom uncertaintyplayground.predplot.mdn_predplot import compare_distributions_mdn\nfrom uncertaintyplayground.predplot.grid_predplot import plot_results_grid\nimport torch\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\n# Load the California Housing dataset\ncalifornia = fetch_california_housing()\n\n# Convert X and y to numpy arrays of float32\nX = np.array(california.data, dtype=np.float32)\ny = np.array(california.target, dtype=np.float32)\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(1)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# SVGPR: Initialize and train a SVGPR model with 100 inducing points\ncalifornia_trainer_svgp = SparseGPTrainer(X_train, y_train, num_inducing_points=100, num_epochs=30, batch_size=512, lr=0.1, patience=3)\ncalifornia_trainer_svgp.train()\n\n# MDN: Initialize and train an MDN model\ncalifornia_trainer_mdn = MDNTrainer(X_train, y_train, num_epochs=100, lr=0.001, dense1_units=50, n_gaussians=10)\ncalifornia_trainer_mdn.train()\n\n# SVPGR: Visualize the SVGPR's predictions for multiple instances\nplot_results_grid(trainer=california_trainer_svgp, compare_func=compare_distributions_svgpr, X_test=X_test, Y_test=y_test, indices=[900, 500], ncols=2)\n\n# MDN: Visualize the MDN's predictions for multiple instances\nplot_results_grid(trainer=california_trainer_mdn, compare_func=compare_distributions_mdn, X_test=X_test, Y_test=y_test, indices=[900, 500], ncols=2)</code></pre> <p>Please note that these models have not been tuned for this dataset, and are only used as an example.</p>"},{"location":"usage/#synthetic-data","title":"Synthetic data","text":"<p>It can be difficult to find truely multi-modal datsets, therefore, to check that MDN works as epected, we can capture the relationships with some simulated data:</p> <pre><code>import torch\nimport numpy as np\nfrom uncertaintyplayground.trainers.mdn_trainer import MDNTrainer\nfrom uncertaintyplayground.utils.generate_data import generate_multi_modal_data\nfrom uncertaintyplayground.predplot.grid_predplot import plot_results_grid\nfrom uncertaintyplayground.predplot.mdn_predplot import compare_distributions_mdn\n\n#-------------------------------------------------------------\n# Example 1: Using MDN model with simulated data\n# Generate simulated data with three modes\nmodes = [\n    {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n    {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n    {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n]\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(1)\n\nnum_samples = 1000\nX = np.random.rand(num_samples, 10)\ny = generate_multi_modal_data(num_samples, modes)\n\n# Train an MDN model with 50 hidden units and 5 Gaussian components\nmdn_trainer = MDNTrainer(X, y, num_epochs=100, lr=0.01, n_gaussians=5, dense1_units=50)\nmdn_trainer.train()\n\n# Visualize the model's predictions and uncertainties for a single instance\n# Note: Make sure that the predictions for the inference are in the same type of float as the training data\nindex_instance = 900\n\n# With an actual y value (diagnostic)\ncompare_distributions_mdn(trainer=mdn_trainer, x_instance=X[index_instance, :],y_actual=y[index_instance], num_samples=1000)\n\n# You can also make any of the plots without a `y` (default set to None)\n# Without an actual y value (pure inference)\ncompare_distributions_mdn(trainer = mdn_trainer, x_instance = X[index_instance, :])</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>models<ul> <li>mdn_model</li> <li>svgp_model</li> </ul> </li> <li>predplot<ul> <li>grid_predplot</li> <li>mdn_predplot</li> <li>svgp_predplot</li> </ul> </li> <li>tests<ul> <li>test_early_stopping</li> <li>test_generate_data</li> <li>test_mdn_model</li> <li>test_mdn_predplot</li> <li>test_mdn_trainer</li> <li>test_svgp_model</li> <li>test_svgp_predplot</li> <li>test_svgp_trainer</li> </ul> </li> <li>trainers<ul> <li>base_trainer</li> <li>mdn_trainer</li> <li>svgp_trainer</li> </ul> </li> <li>utils<ul> <li>early_stopping</li> <li>generate_data</li> </ul> </li> </ul>"},{"location":"reference/models/mdn_model/","title":"mdn_model","text":""},{"location":"reference/models/mdn_model/#models.mdn_model.MDN","title":"<code>MDN</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Mixed Density Network (MDN) model.</p> <p>This model represents a mixture density network for modeling and predicting multi-modal distributions.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of predictors for the first layer of the nueral network.</p> required <code>n_gaussians</code> <code>int</code> <p>Number of Gaussian components in the mixture.</p> required <code>dense1_units</code> <code>int</code> <p>Number of neurons in the first dense layer. Default is 10.</p> <code>10</code> <code>prediction_method</code> <code>str</code> <p>Method for predicting the output distribution. Options are:                      - 'max_weight_mean': Choose the component with the highest weight and return the mean.                      - 'max_weight_sample': Choose a component from the mixture and sample from it.                      - 'average_sample': Draw multiple samples and take the average.</p> <code>'max_weight_sample'</code> <p>Attributes:</p> Name Type Description <code>z_h</code> <code>nn.Sequential</code> <p>Hidden layer of the neural network.</p> <code>z_pi</code> <code>nn.Linear</code> <p>Linear layer for predicting mixture weights.</p> <code>z_mu</code> <code>nn.Linear</code> <p>Linear layer for predicting Gaussian means.</p> <code>z_sigma</code> <code>nn.Linear</code> <p>Linear layer for predicting Gaussian standard deviations.</p> <code>prediction_method</code> <code>str</code> <p>Method for predicting the output distribution.</p> Source code in <code>uncertaintyplayground/models/mdn_model.py</code> <pre><code>class MDN(nn.Module):\n    \"\"\"\n    Mixed Density Network (MDN) model.\n\n    This model represents a mixture density network for modeling and predicting multi-modal distributions.\n\n    Args:\n        input_dim (int): Number of predictors for the first layer of the nueral network.\n        n_gaussians (int): Number of Gaussian components in the mixture.\n        dense1_units (int): Number of neurons in the first dense layer. Default is 10.\n        prediction_method (str): Method for predicting the output distribution. Options are:\n                                 - 'max_weight_mean': Choose the component with the highest weight and return the mean.\n                                 - 'max_weight_sample': Choose a component from the mixture and sample from it.\n                                 - 'average_sample': Draw multiple samples and take the average.\n\n    Attributes:\n        z_h (nn.Sequential): Hidden layer of the neural network.\n        z_pi (nn.Linear): Linear layer for predicting mixture weights.\n        z_mu (nn.Linear): Linear layer for predicting Gaussian means.\n        z_sigma (nn.Linear): Linear layer for predicting Gaussian standard deviations.\n        prediction_method (str): Method for predicting the output distribution.\n    \"\"\"\n\n    def __init__(self, input_dim, n_gaussians, dense1_units = 10, prediction_method='max_weight_sample'):\n        super(MDN, self).__init__()\n        self.z_h = nn.Sequential(\n            nn.Linear(input_dim,dense1_units),\n            nn.Tanh()\n        )\n        self.z_pi = nn.Linear(dense1_units, n_gaussians)\n        self.z_mu = nn.Linear(dense1_units, n_gaussians)\n        self.z_sigma = nn.Linear(dense1_units, n_gaussians)\n        self.prediction_method = prediction_method\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MDN model.\n\n        Computes the parameters (pi, mu, sigma) of the output distribution given the input.\n\n        Args:\n            x (tensor): Input tensor of shape (batch_size, num_features).\n\n        Returns:\n            tuple: A tuple containing the predicted mixture weights, means, and standard deviations.\n        \"\"\"\n        z_h = self.z_h(x)\n        pi = F.softmax(self.z_pi(z_h), -1)\n        mu = self.z_mu(z_h)\n        sigma = torch.exp(self.z_sigma(z_h))\n        return pi, mu, sigma\n\n    def sample(self, x, num_samples=100):\n        \"\"\"\n        Generate samples from the output distribution given the input.\n\n        Args:\n            x (tensor): Input tensor of shape (batch_size, num_features).\n            num_samples (int): Number of samples to generate. Default is 100.\n\n        Returns:\n            tensor: A tensor of shape (batch_size,) containing the generated samples.\n        \"\"\"\n        pi, mu, sigma = self.forward(x)\n\n        if self.prediction_method == 'max_weight_mean':\n            # Choose component with the highest weight\n            pis = torch.argmax(pi, dim=1)\n            # Return the mean of the chosen component\n            sample = mu[torch.arange(mu.size(0)), pis]\n\n        elif self.prediction_method == 'max_weight_sample':\n            # Choose component from the mixture\n            categorical = torch.distributions.Categorical(pi)\n            pis = list(categorical.sample().data)\n            # Sample from the chosen component\n            sample = Variable(sigma.data.new(sigma.size(0)).normal_())\n            for i in range(sigma.size(0)):\n                sample[i] = sample[i] * sigma[i, pis[i]] + mu[i, pis[i]]\n\n        elif self.prediction_method == 'average_sample':\n            # Draw multiple samples and take the average\n            samples = []\n            for _ in range(num_samples):\n                # Choose component from the mixture\n                categorical = torch.distributions.Categorical(pi)\n                pis = list(categorical.sample().data)\n                # Sample from the chosen component\n                sample = Variable(sigma.data.new(sigma.size(0)).normal_())\n                for i in range(sigma.size(0)):\n                    sample[i] = sample[i] * sigma[i, pis[i]] + mu[i, pis[i]]\n                samples.append(sample)\n            sample = torch.mean(torch.stack(samples), dim=0)\n\n        else:\n            raise ValueError(f\"Invalid prediction method: {self.prediction_method}\")\n\n        return sample</code></pre>"},{"location":"reference/models/mdn_model/#models.mdn_model.MDN.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the MDN model.</p> <p>Computes the parameters (pi, mu, sigma) of the output distribution given the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tensor</code> <p>Input tensor of shape (batch_size, num_features).</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the predicted mixture weights, means, and standard deviations.</p> Source code in <code>uncertaintyplayground/models/mdn_model.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the MDN model.\n\n    Computes the parameters (pi, mu, sigma) of the output distribution given the input.\n\n    Args:\n        x (tensor): Input tensor of shape (batch_size, num_features).\n\n    Returns:\n        tuple: A tuple containing the predicted mixture weights, means, and standard deviations.\n    \"\"\"\n    z_h = self.z_h(x)\n    pi = F.softmax(self.z_pi(z_h), -1)\n    mu = self.z_mu(z_h)\n    sigma = torch.exp(self.z_sigma(z_h))\n    return pi, mu, sigma</code></pre>"},{"location":"reference/models/mdn_model/#models.mdn_model.MDN.sample","title":"<code>sample(x, num_samples=100)</code>","text":"<p>Generate samples from the output distribution given the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tensor</code> <p>Input tensor of shape (batch_size, num_features).</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to generate. Default is 100.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>tensor</code> <p>A tensor of shape (batch_size,) containing the generated samples.</p> Source code in <code>uncertaintyplayground/models/mdn_model.py</code> <pre><code>def sample(self, x, num_samples=100):\n    \"\"\"\n    Generate samples from the output distribution given the input.\n\n    Args:\n        x (tensor): Input tensor of shape (batch_size, num_features).\n        num_samples (int): Number of samples to generate. Default is 100.\n\n    Returns:\n        tensor: A tensor of shape (batch_size,) containing the generated samples.\n    \"\"\"\n    pi, mu, sigma = self.forward(x)\n\n    if self.prediction_method == 'max_weight_mean':\n        # Choose component with the highest weight\n        pis = torch.argmax(pi, dim=1)\n        # Return the mean of the chosen component\n        sample = mu[torch.arange(mu.size(0)), pis]\n\n    elif self.prediction_method == 'max_weight_sample':\n        # Choose component from the mixture\n        categorical = torch.distributions.Categorical(pi)\n        pis = list(categorical.sample().data)\n        # Sample from the chosen component\n        sample = Variable(sigma.data.new(sigma.size(0)).normal_())\n        for i in range(sigma.size(0)):\n            sample[i] = sample[i] * sigma[i, pis[i]] + mu[i, pis[i]]\n\n    elif self.prediction_method == 'average_sample':\n        # Draw multiple samples and take the average\n        samples = []\n        for _ in range(num_samples):\n            # Choose component from the mixture\n            categorical = torch.distributions.Categorical(pi)\n            pis = list(categorical.sample().data)\n            # Sample from the chosen component\n            sample = Variable(sigma.data.new(sigma.size(0)).normal_())\n            for i in range(sigma.size(0)):\n                sample[i] = sample[i] * sigma[i, pis[i]] + mu[i, pis[i]]\n            samples.append(sample)\n        sample = torch.mean(torch.stack(samples), dim=0)\n\n    else:\n        raise ValueError(f\"Invalid prediction method: {self.prediction_method}\")\n\n    return sample</code></pre>"},{"location":"reference/models/mdn_model/#models.mdn_model.mdn_loss","title":"<code>mdn_loss(y, mu, sigma, pi)</code>","text":"<p>Compute the MDN loss.</p> <p>Calculates the negative log-likelihood of the target variable given the predicted parameters of the mixture.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>tensor</code> <p>Target tensor of shape (batch_size,).</p> required <code>mu</code> <code>tensor</code> <p>Predicted means tensor of shape (batch_size, n_gaussians).</p> required <code>sigma</code> <code>tensor</code> <p>Predicted standard deviations tensor of shape (batch_size, n_gaussians).</p> required <code>pi</code> <code>tensor</code> <p>Predicted mixture weights tensor of shape (batch_size, n_gaussians).</p> required <p>Returns:</p> Name Type Description <code>tensor</code> <p>The computed loss.</p> Source code in <code>uncertaintyplayground/models/mdn_model.py</code> <pre><code>def mdn_loss(y, mu, sigma, pi):\n    \"\"\"\n    Compute the MDN loss.\n\n    Calculates the negative log-likelihood of the target variable given the predicted parameters of the mixture.\n\n    Args:\n        y (tensor): Target tensor of shape (batch_size,).\n        mu (tensor): Predicted means tensor of shape (batch_size, n_gaussians).\n        sigma (tensor): Predicted standard deviations tensor of shape (batch_size, n_gaussians).\n        pi (tensor): Predicted mixture weights tensor of shape (batch_size, n_gaussians).\n\n    Returns:\n        tensor: The computed loss.\n    \"\"\"\n    m = Normal(loc=mu, scale=sigma)\n    log_prob = m.log_prob(y.unsqueeze(1))\n    log_mix = torch.log(pi) + log_prob\n    return -torch.logsumexp(log_mix, dim=1).mean()</code></pre>"},{"location":"reference/models/svgp_model/","title":"svgp_model","text":""},{"location":"reference/models/svgp_model/#models.svgp_model.SVGP","title":"<code>SVGP</code>","text":"<p>         Bases: <code>gpytorch.models.ApproximateGP</code></p> <p>Stochastic Variational Gaussian Process (SVGP) Regression Model.</p> <p>A scalable Gaussian Process (GP) model based on stochastic variational inference. Inherits from the gpytorch.models.ApproximateGP class.</p> <p>Parameters:</p> Name Type Description Default <code>inducing_points</code> <code>torch.Tensor</code> <p>Inducing points tensor.</p> required <code>dtype</code> <code>torch.dtype</code> <p>Data type of the model. Defaults to torch.float32.</p> <code>torch.float32</code> <p>Attributes:</p> Name Type Description <code>mean_module</code> <code>gpytorch.means.ConstantMean</code> <p>Constant mean module.</p> <code>covar_module</code> <code>gpytorch.kernels.ScaleKernel</code> <p>Scaled RBF kernel.</p> Source code in <code>uncertaintyplayground/models/svgp_model.py</code> <pre><code>class SVGP(gpytorch.models.ApproximateGP):\n    \"\"\"\n    Stochastic Variational Gaussian Process (SVGP) Regression Model.\n\n    A scalable Gaussian Process (GP) model based on stochastic variational inference.\n    Inherits from the gpytorch.models.ApproximateGP class.\n\n    Args:\n        inducing_points (torch.Tensor): Inducing points tensor.\n        dtype (torch.dtype, optional): Data type of the model. Defaults to torch.float32.\n\n    Attributes:\n        mean_module (gpytorch.means.ConstantMean): Constant mean module.\n        covar_module (gpytorch.kernels.ScaleKernel): Scaled RBF kernel.\n    \"\"\"\n\n    def __init__(self, inducing_points, dtype=torch.float32):\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            inducing_points.size(0), dtype=dtype\n        )\n        variational_strategy = gpytorch.variational.VariationalStrategy(\n            self,\n            inducing_points,\n            variational_distribution,\n            learn_inducing_locations=True,\n        ).to(dtype)\n\n        super().__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean(dtype=dtype)\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the SVGPRegressionModel.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            gpytorch.distributions.MultivariateNormal: Multivariate normal distribution with the given mean and covariance.\n        \"\"\"\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)</code></pre>"},{"location":"reference/models/svgp_model/#models.svgp_model.SVGP.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass for the SVGPRegressionModel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <p>gpytorch.distributions.MultivariateNormal: Multivariate normal distribution with the given mean and covariance.</p> Source code in <code>uncertaintyplayground/models/svgp_model.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass for the SVGPRegressionModel.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        gpytorch.distributions.MultivariateNormal: Multivariate normal distribution with the given mean and covariance.\n    \"\"\"\n    mean_x = self.mean_module(x)\n    covar_x = self.covar_module(x)\n    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)</code></pre>"},{"location":"reference/predplot/grid_predplot/","title":"grid_predplot","text":""},{"location":"reference/predplot/grid_predplot/#predplot.grid_predplot.DisablePlotDisplay","title":"<code>DisablePlotDisplay</code>","text":"<p>Context manager to disable the display of matplotlib plots.</p> Source code in <code>uncertaintyplayground/predplot/grid_predplot.py</code> <pre><code>class DisablePlotDisplay:\n    \"\"\"\n    Context manager to disable the display of matplotlib plots.\n    \"\"\"\n\n    def __enter__(self):\n        plt.ioff()  # Turn off interactive mode\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        plt.close()  # Close the plot</code></pre>"},{"location":"reference/predplot/grid_predplot/#predplot.grid_predplot.plot_results_grid","title":"<code>plot_results_grid(trainer, compare_func, X_test, indices, Y_test=None, ncols=2, dtype=np.float32)</code>","text":"<p>Plot a grid of comparison plots (minimum 2) for a set of test instances.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>object</code> <p>The trained MDNTrainer or SparseGPTrainer instance.</p> required <code>compare_func</code> <code>function</code> <p>Function to compare distributions (compare_distributions for MDN or compare_distributions_svgpr for SVGPR).</p> required <code>X_test</code> <code>np.ndarray</code> <p>The test input data of shape (num_samples, num_features).</p> required <code>indices</code> <code>list</code> <p>The indices of the instances to plot.</p> required <code>ncols</code> <code>int</code> <p>Number of columns in the grid. Default is 2.</p> <code>2</code> <code>Y_test</code> <code>np.ndarray</code> <p>The test target data of shape (num_samples,). Default is None.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>Data type to use for plotting. Default is np.float32.</p> <code>np.float32</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>uncertaintyplayground/predplot/grid_predplot.py</code> <pre><code>def plot_results_grid(trainer, compare_func, X_test, indices, Y_test= None, ncols=2, dtype=np.float32):\n    \"\"\"\n    Plot a grid of comparison plots (minimum 2) for a set of test instances.\n\n    Args:\n        trainer (object): The trained MDNTrainer or SparseGPTrainer instance.\n        compare_func (function): Function to compare distributions (compare_distributions for MDN or compare_distributions_svgpr for SVGPR).\n        X_test (np.ndarray): The test input data of shape (num_samples, num_features).\n        indices (list): The indices of the instances to plot.\n        ncols (int, optional): Number of columns in the grid. Default is 2.\n        Y_test (np.ndarray): The test target data of shape (num_samples,). Default is None.\n        dtype (np.dtype, optional): Data type to use for plotting. Default is np.float32.\n\n    Returns:\n        None\n    \"\"\"\n    num_instances = len(indices)\n    nrows = (num_instances - 1) // ncols + 1\n\n    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 5 * nrows))\n\n    for i, ax in zip(indices, axes.flat):\n        x_instance = X_test[i].astype(dtype)\n        if Y_test is None:\n            y_actual = None\n        else:\n            y_actual = Y_test[i].astype(dtype)\n        compare_func(trainer, x_instance, y_actual, ax=ax)\n        ax.set_title(f\"Test Instance: {i}\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.grid(axis='y', alpha=0.75)\n\n    # Remove empty subplots\n    if num_instances &lt; nrows * ncols:\n        for ax in axes.flat[num_instances:]:\n            ax.remove()\n\n    plt.tight_layout()\n    plt.show()</code></pre>"},{"location":"reference/predplot/mdn_predplot/","title":"mdn_predplot","text":""},{"location":"reference/predplot/mdn_predplot/#predplot.mdn_predplot.compare_distributions_mdn","title":"<code>compare_distributions_mdn(trainer, x_instance, y_actual=None, num_samples=10000, ax=None, dtype=np.float32)</code>","text":"<p>Compare the actual and predicted outcome value/distributions for the MDN model.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>MDNTrainer</code> <p>The trained MDNTrainer instance.</p> required <code>x_instance</code> <code>np.ndarray or torch.Tensor</code> <p>The instance for which to predict the outcome distribution.</p> required <code>y_actual</code> <code>float or np.ndarray</code> <p>The actual outcome(s). If a single value, plot as a vertical line.                                       If an array or list, plot as a KDE. If None, don't plot actual outcome.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>The number of samples to generate from the predicted and actual distributions.                          Default is 10000.</p> <code>10000</code> <code>ax</code> <code>matplotlib.axes.Axes</code> <p>The axes on which to plot. If None, create a new figure.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>Data type to use for plotting. Default is np.float32.</p> <code>np.float32</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>uncertaintyplayground/predplot/mdn_predplot.py</code> <pre><code>def compare_distributions_mdn(trainer, x_instance, y_actual=None, num_samples=10000, ax=None, dtype=np.float32):\n    \"\"\"\n    Compare the actual and predicted outcome value/distributions for the MDN model.\n\n    Args:\n        trainer (MDNTrainer): The trained MDNTrainer instance.\n        x_instance (np.ndarray or torch.Tensor): The instance for which to predict the outcome distribution.\n        y_actual (float or np.ndarray, optional): The actual outcome(s). If a single value, plot as a vertical line.\n                                                  If an array or list, plot as a KDE. If None, don't plot actual outcome.\n        num_samples (int, optional): The number of samples to generate from the predicted and actual distributions.\n                                     Default is 10000.\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot. If None, create a new figure.\n        dtype (np.dtype, optional): Data type to use for plotting. Default is np.float32.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure it passes the unit testing\n    warnings.warn(\"This is a UserWarning\")\n\n    # Ensure x_instance is a 2D array\n    if x_instance.ndim == 1:\n        x_instance = np.expand_dims(x_instance, axis=0)\n\n    # Change the prediction instance to the desired data type\n    x_instance = x_instance.astype(dtype)\n\n    # Get the predicted parameters of the mixture distribution\n    pi, mu, sigma, pred = trainer.predict_with_uncertainty(x_instance)\n\n    # Generate samples from the predicted distribution\n    predicted_samples = []\n\n    print (\"The actual input was: \",x_instance, \"\\n\",\n           \"The ground truth was: \",y_actual, \"\\n\",\n           \"The predicted weights are: \",pi, \"\\n\", \n           \"The predicted means are: \",mu, \"\\n\",\n           \"The predicted sigmas are: \",sigma, \"\\n\",\n           \"The final prediction is: \",pred, \"\\n\")\n\n    for _ in range(num_samples):\n        # Choose Gaussian\n        idx = np.random.choice(np.arange(len(pi[0])), p=pi[0])\n        # Sample from Gaussian\n        sample = np.random.normal(mu[0, idx], sigma[0, idx])\n        predicted_samples.append(sample)\n\n    # Plot KDE of predicted samples\n    if ax is None:\n        sns.kdeplot(predicted_samples, fill=True, color=\"r\", label=\"Predicted distribution\")\n        plt.axvline(pred[0], color=\"r\", linestyle=\"--\", label=\"Predicted value\")\n    else:\n        sns.kdeplot(predicted_samples, fill=True, color=\"r\", label=\"Predicted distribution\", ax=ax)\n        ax.axvline(pred[0], color=\"r\", linestyle=\"--\", label=\"Predicted value\")\n\n    # Plot the actual value(s)\n    if y_actual is not None:\n        if np.isscalar(y_actual):\n            if ax is None:\n                plt.axvline(y_actual, color=\"b\", linestyle=\"--\", label=\"Ground truth\")\n            else:\n                ax.axvline(y_actual, color=\"b\", linestyle=\"--\", label=\"Ground truth\")\n        else:\n            if ax is None:\n                sns.kdeplot(y_actual, fill=True, color=\"b\", label=\"Actual\")\n            else:\n                sns.kdeplot(y_actual, fill=True, color=\"b\", label=\"Actual\", ax=ax)\n\n    # Set an option when doing multiple plots\n    if ax is None:\n        plt.xlabel('Value')\n        plt.ylabel('Density')\n        plt.legend()\n        plt.grid(axis='y', alpha=0.75)\n        plt.show()</code></pre>"},{"location":"reference/predplot/svgp_predplot/","title":"svgp_predplot","text":""},{"location":"reference/predplot/svgp_predplot/#predplot.svgp_predplot.compare_distributions_svgpr","title":"<code>compare_distributions_svgpr(trainer, x_instance, y_actual=None, num_samples=10000, ax=None, dtype=np.float32)</code>","text":"<p>Compare the actual and predicted outcome value/distributions for the SVGPR model.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>SVGPRTrainer</code> <p>The trained SVGPRTrainer instance.</p> required <code>x_instance</code> <code>np.ndarray</code> <p>The instance for which to predict the outcome distribution.</p> required <code>y_actual</code> <code>float or np.ndarray</code> <p>The actual outcome. If a single value, plot as a vertical line.                                       If an array or list, plot as a KDE. If None, don't plot actual outcome.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>The number of samples to generate from the predicted distribution.                          Default is 10000.</p> <code>10000</code> <code>ax</code> <code>matplotlib.axes.Axes</code> <p>The axes on which to plot. If None, create a new figure.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>Data type to use for plotting. Default is np.float32.</p> <code>np.float32</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>uncertaintyplayground/predplot/svgp_predplot.py</code> <pre><code>def compare_distributions_svgpr(trainer, x_instance, y_actual=None, num_samples=10000, ax=None, dtype=np.float32):\n    \"\"\"\n    Compare the actual and predicted outcome value/distributions for the SVGPR model.\n\n    Args:\n        trainer (SVGPRTrainer): The trained SVGPRTrainer instance.\n        x_instance (np.ndarray): The instance for which to predict the outcome distribution.\n        y_actual (float or np.ndarray, optional): The actual outcome. If a single value, plot as a vertical line.\n                                                  If an array or list, plot as a KDE. If None, don't plot actual outcome.\n        num_samples (int, optional): The number of samples to generate from the predicted distribution.\n                                     Default is 10000.\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot. If None, create a new figure.\n        dtype (np.dtype, optional): Data type to use for plotting. Default is np.float32.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure x_instance is a 2D array\n    if x_instance.ndim == 1:\n        x_instance = np.expand_dims(x_instance, axis=0)\n\n    # Change the prediction instance to the desired data type\n    x_instance = x_instance.astype(dtype)\n\n    # Get the predicted mean and standard deviation\n    mu, sigma = trainer.predict_with_uncertainty(x_instance)\n\n    # Generate samples from the predicted distribution\n    predicted_samples = np.random.normal(mu, sigma, num_samples)\n\n    # Plot KDE of predicted samples\n    if ax is None:\n        sns.kdeplot(predicted_samples, fill=True, color=\"r\", label=\"Predicted distribution\")\n        plt.axvline(mu, color=\"r\", linestyle=\"--\", label=\"Predicted value\")\n    else:\n        sns.kdeplot(predicted_samples, fill=True, color=\"r\", label=\"Predicted distribution\", ax=ax)\n        ax.axvline(mu, color=\"r\", linestyle=\"--\", label=\"Predicted value\")\n\n    # Plot the actual value\n    if y_actual is not None:\n        if ax is None:\n            plt.axvline(y_actual, color=\"b\", linestyle=\"--\", label=\"Actual value\")\n        else:\n            ax.axvline(y_actual, color=\"b\", linestyle=\"--\", label=\"Actual value\")\n\n    # Set an option when doing multiple plots\n    if ax is None:\n        plt.xlabel('Value')\n        plt.ylabel('Density')\n        plt.legend()\n        plt.grid(axis='y', alpha=0.75)\n        plt.show()</code></pre>"},{"location":"reference/tests/test_early_stopping/","title":"test_early_stopping","text":""},{"location":"reference/tests/test_early_stopping/#tests.test_early_stopping.TestEarlyStopping","title":"<code>TestEarlyStopping</code>","text":"<p>         Bases: <code>unittest.TestCase</code></p> <p>Unit tests for EarlyStopping class.</p> Source code in <code>uncertaintyplayground/tests/test_early_stopping.py</code> <pre><code>class TestEarlyStopping(unittest.TestCase):\n    \"\"\"\n    Unit tests for EarlyStopping class.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Test fixture setup method.\n        \"\"\"\n        self.early_stopping = EarlyStopping()\n\n    def test_init(self):\n        \"\"\"\n        Test case for EarlyStopping initialization.\n        \"\"\"\n        self.assertIsInstance(self.early_stopping, EarlyStopping)\n        self.assertEqual(self.early_stopping.counter, 0)\n        self.assertEqual(self.early_stopping.best_val_metric, np.inf)\n\n    def test_call(self):\n        \"\"\"\n        Test case for call method in EarlyStopping.\n        \"\"\"\n        model = torch.nn.Linear(1, 1)  # simple model for testing\n        val_metric = 10\n        self.early_stopping(val_metric, model)\n        self.assertEqual(self.early_stopping.best_val_metric, 10)\n        self.assertEqual(self.early_stopping.counter, 0)\n\n        val_metric = 20\n        self.early_stopping(val_metric, model)\n        self.assertEqual(self.early_stopping.best_val_metric, 10)\n        self.assertEqual(self.early_stopping.counter, 1)</code></pre>"},{"location":"reference/tests/test_early_stopping/#tests.test_early_stopping.TestEarlyStopping.setUp","title":"<code>setUp()</code>","text":"<p>Test fixture setup method.</p> Source code in <code>uncertaintyplayground/tests/test_early_stopping.py</code> <pre><code>def setUp(self):\n    \"\"\"\n    Test fixture setup method.\n    \"\"\"\n    self.early_stopping = EarlyStopping()</code></pre>"},{"location":"reference/tests/test_early_stopping/#tests.test_early_stopping.TestEarlyStopping.test_call","title":"<code>test_call()</code>","text":"<p>Test case for call method in EarlyStopping.</p> Source code in <code>uncertaintyplayground/tests/test_early_stopping.py</code> <pre><code>def test_call(self):\n    \"\"\"\n    Test case for call method in EarlyStopping.\n    \"\"\"\n    model = torch.nn.Linear(1, 1)  # simple model for testing\n    val_metric = 10\n    self.early_stopping(val_metric, model)\n    self.assertEqual(self.early_stopping.best_val_metric, 10)\n    self.assertEqual(self.early_stopping.counter, 0)\n\n    val_metric = 20\n    self.early_stopping(val_metric, model)\n    self.assertEqual(self.early_stopping.best_val_metric, 10)\n    self.assertEqual(self.early_stopping.counter, 1)</code></pre>"},{"location":"reference/tests/test_early_stopping/#tests.test_early_stopping.TestEarlyStopping.test_init","title":"<code>test_init()</code>","text":"<p>Test case for EarlyStopping initialization.</p> Source code in <code>uncertaintyplayground/tests/test_early_stopping.py</code> <pre><code>def test_init(self):\n    \"\"\"\n    Test case for EarlyStopping initialization.\n    \"\"\"\n    self.assertIsInstance(self.early_stopping, EarlyStopping)\n    self.assertEqual(self.early_stopping.counter, 0)\n    self.assertEqual(self.early_stopping.best_val_metric, np.inf)</code></pre>"},{"location":"reference/tests/test_generate_data/","title":"test_generate_data","text":""},{"location":"reference/tests/test_generate_data/#tests.test_generate_data.test_generate_multi_modal_data","title":"<code>test_generate_multi_modal_data()</code>","text":"<p>Unit test for <code>generate_multi_modal_data</code> function.</p> <p>This test checks whether the function returns an array of correct size and also if the data follows the expected distribution by checking whether the mean and standard deviation are close to the expected values.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the returned array is not of the correct size.</p> <code>AssertionError</code> <p>If the mean or standard deviation of the generated data is too far from the expected value.</p> Source code in <code>uncertaintyplayground/tests/test_generate_data.py</code> <pre><code>def test_generate_multi_modal_data():\n    \"\"\"\n    Unit test for `generate_multi_modal_data` function.\n\n    This test checks whether the function returns an array of correct size and also if the data\n    follows the expected distribution by checking whether the mean and standard deviation are\n    close to the expected values.\n\n    Raises:\n        AssertionError: If the returned array is not of the correct size.\n        AssertionError: If the mean or standard deviation of the generated data is too far from the expected value.\n    \"\"\"\n    num_samples = 10000\n    modes = [\n        {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n        {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n        {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n    ]\n\n    y = generate_multi_modal_data(num_samples, modes)\n\n    assert len(y) == num_samples, \"The generated data does not have the correct number of samples.\"\n\n    # Check if the mean and std deviation are close to expected values\n    for mode in modes:\n        mode_samples = [sample for sample in y if abs(sample-mode['mean']) &lt; 3*mode['std_dev']]\n        assert abs(np.mean(mode_samples)-mode['mean']) &lt; 0.1, f\"For mode with mean {mode['mean']}, generated data does not have correct mean.\"\n        assert abs(np.std(mode_samples)-mode['std_dev']) &lt; 0.1, f\"For mode with mean {mode['mean']}, generated data does not have correct std deviation.\"</code></pre>"},{"location":"reference/tests/test_mdn_model/","title":"test_mdn_model","text":""},{"location":"reference/tests/test_mdn_model/#tests.test_mdn_model.TestMDN","title":"<code>TestMDN</code>","text":"<p>         Bases: <code>unittest.TestCase</code></p> <p>Tests for the class MDN</p> Source code in <code>uncertaintyplayground/tests/test_mdn_model.py</code> <pre><code>class TestMDN(unittest.TestCase):\n    \"\"\"Tests for the class MDN\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up a test fixture with input_dim = 20, dense1_units  = 10, n_gaussians = 3\"\"\"\n        self.input_dim = 20\n        self.dense1_units = 10\n        self.n_gaussians = 3\n        self.mdn = MDN(input_dim=self.input_dim, n_gaussians=self.n_gaussians, dense1_units=self.dense1_units)\n\n    def test_init(self):\n        \"\"\"Test that the MDN is initialized properly\"\"\"\n        self.assertEqual(self.mdn.z_h[0].in_features, self.input_dim)\n        self.assertEqual(self.mdn.z_h[0].out_features, self.dense1_units)\n        self.assertEqual(self.mdn.z_pi.in_features, self.dense1_units)\n        self.assertEqual(self.mdn.z_pi.out_features, self.n_gaussians)\n\n    def test_forward(self):\n        \"\"\"Test the forward function with a tensor of shape (1, 20)\"\"\"\n        x = torch.rand((1, self.input_dim))\n        pi, mu, sigma = self.mdn.forward(x)\n        self.assertEqual(pi.shape, (1, self.n_gaussians))\n        self.assertEqual(mu.shape, (1, self.n_gaussians))\n        self.assertEqual(sigma.shape, (1, self.n_gaussians))\n\n    def test_sample(self):\n        \"\"\"Test the sample function with a tensor of shape (1, 20)\"\"\"\n        x = torch.rand((1, self.input_dim))\n        sample = self.mdn.sample(x)\n        self.assertEqual(sample.shape, (1,))</code></pre>"},{"location":"reference/tests/test_mdn_model/#tests.test_mdn_model.TestMDN.setUp","title":"<code>setUp()</code>","text":"<p>Set up a test fixture with input_dim = 20, dense1_units  = 10, n_gaussians = 3</p> Source code in <code>uncertaintyplayground/tests/test_mdn_model.py</code> <pre><code>def setUp(self):\n    \"\"\"Set up a test fixture with input_dim = 20, dense1_units  = 10, n_gaussians = 3\"\"\"\n    self.input_dim = 20\n    self.dense1_units = 10\n    self.n_gaussians = 3\n    self.mdn = MDN(input_dim=self.input_dim, n_gaussians=self.n_gaussians, dense1_units=self.dense1_units)</code></pre>"},{"location":"reference/tests/test_mdn_model/#tests.test_mdn_model.TestMDN.test_forward","title":"<code>test_forward()</code>","text":"<p>Test the forward function with a tensor of shape (1, 20)</p> Source code in <code>uncertaintyplayground/tests/test_mdn_model.py</code> <pre><code>def test_forward(self):\n    \"\"\"Test the forward function with a tensor of shape (1, 20)\"\"\"\n    x = torch.rand((1, self.input_dim))\n    pi, mu, sigma = self.mdn.forward(x)\n    self.assertEqual(pi.shape, (1, self.n_gaussians))\n    self.assertEqual(mu.shape, (1, self.n_gaussians))\n    self.assertEqual(sigma.shape, (1, self.n_gaussians))</code></pre>"},{"location":"reference/tests/test_mdn_model/#tests.test_mdn_model.TestMDN.test_init","title":"<code>test_init()</code>","text":"<p>Test that the MDN is initialized properly</p> Source code in <code>uncertaintyplayground/tests/test_mdn_model.py</code> <pre><code>def test_init(self):\n    \"\"\"Test that the MDN is initialized properly\"\"\"\n    self.assertEqual(self.mdn.z_h[0].in_features, self.input_dim)\n    self.assertEqual(self.mdn.z_h[0].out_features, self.dense1_units)\n    self.assertEqual(self.mdn.z_pi.in_features, self.dense1_units)\n    self.assertEqual(self.mdn.z_pi.out_features, self.n_gaussians)</code></pre>"},{"location":"reference/tests/test_mdn_model/#tests.test_mdn_model.TestMDN.test_sample","title":"<code>test_sample()</code>","text":"<p>Test the sample function with a tensor of shape (1, 20)</p> Source code in <code>uncertaintyplayground/tests/test_mdn_model.py</code> <pre><code>def test_sample(self):\n    \"\"\"Test the sample function with a tensor of shape (1, 20)\"\"\"\n    x = torch.rand((1, self.input_dim))\n    sample = self.mdn.sample(x)\n    self.assertEqual(sample.shape, (1,))</code></pre>"},{"location":"reference/tests/test_mdn_predplot/","title":"test_mdn_predplot","text":""},{"location":"reference/tests/test_mdn_predplot/#tests.test_mdn_predplot.TestMDNPlots","title":"<code>TestMDNPlots</code>","text":"<p>         Bases: <code>unittest.TestCase</code></p> <p>This test class provides unit tests for the compare_distributions_mdn and plot_results_grid functions for MDN model.</p> Source code in <code>uncertaintyplayground/tests/test_mdn_predplot.py</code> <pre><code>class TestMDNPlots(unittest.TestCase):\n    \"\"\"\n    This test class provides unit tests for the compare_distributions_mdn and plot_results_grid functions for MDN model.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Sets up the testing environment for each test method.\n        \"\"\"\n        self.modes = [\n            {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n            {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n            {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n        ]\n\n        torch.manual_seed(1)\n        np.random.seed(42)\n        self.num_samples = 1000\n        self.X_test = np.random.rand(self.num_samples, 20)\n        self.Y_test = generate_multi_modal_data(self.num_samples, self.modes)\n\n        self.mdn_trainer = MDNTrainer(self.X_test, self.Y_test, num_epochs=5, lr=0.01, dense1_units=5, n_gaussians=3)\n        self.mdn_trainer.train()\n\n    def test_compare_distributions_mdn(self):\n        \"\"\"\n        Tests the compare_distributions function for the MDN model.\n        \"\"\"\n        index_instance = 900\n\n        with DisablePlotDisplay():\n            compare_distributions_mdn(self.mdn_trainer, x_instance = self.X_test[index_instance, :], y_actual=self.Y_test[index_instance])\n\n    def test_plot_results_grid_mdn(self):\n        \"\"\"\n        Tests the plot_results_grid function with MDN model.\n        \"\"\"\n        indices = [900, 100]  # Example indices\n\n        # Testing with existing actual values\n        with DisablePlotDisplay():\n            plot_results_grid(self.mdn_trainer, compare_distributions_mdn, self.X_test, indices, self.Y_test)\n        # Testing with no y values\n        with DisablePlotDisplay():\n            plot_results_grid(self.mdn_trainer, compare_distributions_mdn, self.X_test, indices, None)</code></pre>"},{"location":"reference/tests/test_mdn_predplot/#tests.test_mdn_predplot.TestMDNPlots.setUp","title":"<code>setUp()</code>","text":"<p>Sets up the testing environment for each test method.</p> Source code in <code>uncertaintyplayground/tests/test_mdn_predplot.py</code> <pre><code>def setUp(self):\n    \"\"\"\n    Sets up the testing environment for each test method.\n    \"\"\"\n    self.modes = [\n        {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n        {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n        {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n    ]\n\n    torch.manual_seed(1)\n    np.random.seed(42)\n    self.num_samples = 1000\n    self.X_test = np.random.rand(self.num_samples, 20)\n    self.Y_test = generate_multi_modal_data(self.num_samples, self.modes)\n\n    self.mdn_trainer = MDNTrainer(self.X_test, self.Y_test, num_epochs=5, lr=0.01, dense1_units=5, n_gaussians=3)\n    self.mdn_trainer.train()</code></pre>"},{"location":"reference/tests/test_mdn_predplot/#tests.test_mdn_predplot.TestMDNPlots.test_compare_distributions_mdn","title":"<code>test_compare_distributions_mdn()</code>","text":"<p>Tests the compare_distributions function for the MDN model.</p> Source code in <code>uncertaintyplayground/tests/test_mdn_predplot.py</code> <pre><code>def test_compare_distributions_mdn(self):\n    \"\"\"\n    Tests the compare_distributions function for the MDN model.\n    \"\"\"\n    index_instance = 900\n\n    with DisablePlotDisplay():\n        compare_distributions_mdn(self.mdn_trainer, x_instance = self.X_test[index_instance, :], y_actual=self.Y_test[index_instance])</code></pre>"},{"location":"reference/tests/test_mdn_predplot/#tests.test_mdn_predplot.TestMDNPlots.test_plot_results_grid_mdn","title":"<code>test_plot_results_grid_mdn()</code>","text":"<p>Tests the plot_results_grid function with MDN model.</p> Source code in <code>uncertaintyplayground/tests/test_mdn_predplot.py</code> <pre><code>def test_plot_results_grid_mdn(self):\n    \"\"\"\n    Tests the plot_results_grid function with MDN model.\n    \"\"\"\n    indices = [900, 100]  # Example indices\n\n    # Testing with existing actual values\n    with DisablePlotDisplay():\n        plot_results_grid(self.mdn_trainer, compare_distributions_mdn, self.X_test, indices, self.Y_test)\n    # Testing with no y values\n    with DisablePlotDisplay():\n        plot_results_grid(self.mdn_trainer, compare_distributions_mdn, self.X_test, indices, None)</code></pre>"},{"location":"reference/tests/test_mdn_trainer/","title":"test_mdn_trainer","text":""},{"location":"reference/tests/test_mdn_trainer/#tests.test_mdn_trainer.TestMDNTrainer","title":"<code>TestMDNTrainer</code>","text":"<p>         Bases: <code>unittest.TestCase</code></p> <p>This test class provides unit tests for the MDNTrainer class.</p> Source code in <code>uncertaintyplayground/tests/test_mdn_trainer.py</code> <pre><code>class TestMDNTrainer(unittest.TestCase):\n    \"\"\"\n    This test class provides unit tests for the MDNTrainer class.\n\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Sets up the testing environment for each test method.\n        \"\"\"\n        self.modes = [\n            {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n            {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n            {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n        ]\n\n        torch.manual_seed(1)\n        np.random.seed(42)\n        self.num_samples = 100\n        self.X = np.random.rand(self.num_samples, 20)\n        self.y = generate_multi_modal_data(self.num_samples, self.modes)\n\n    def test_train(self):\n        \"\"\"\n        Tests the train method of the MDNTrainer class.\n        \"\"\"\n        trainer = MDNTrainer(self.X, self.y, num_epochs=3, lr=0.01, dense1_units=5, n_gaussians=3, dtype = torch.float32)\n        trainer.train()\n\n    def test_predict_with_uncertainty(self):\n        \"\"\"\n        Tests the predict_with_uncertainty method of the MDNTrainer class.\n        \"\"\"\n        trainer = MDNTrainer(self.X, self.y, num_epochs=2, lr=0.01, dense1_units=5, n_gaussians=3)\n        trainer.train()\n\n        # Generate a test instance\n        test_instance = np.random.rand(1, 20)\n        test_instance = test_instance.astype(np.float32)\n\n        pi, mu, sigma, sample = trainer.predict_with_uncertainty(test_instance)\n        # Add assertions based on the expected behavior of the predict_with_uncertainty function\n        self.assertEqual(pi.shape, (1, 3))\n        self.assertEqual(mu.shape, (1, 3))\n        self.assertEqual(sigma.shape, (1, 3))\n        self.assertIsInstance(sample, np.ndarray)</code></pre>"},{"location":"reference/tests/test_mdn_trainer/#tests.test_mdn_trainer.TestMDNTrainer.setUp","title":"<code>setUp()</code>","text":"<p>Sets up the testing environment for each test method.</p> Source code in <code>uncertaintyplayground/tests/test_mdn_trainer.py</code> <pre><code>def setUp(self):\n    \"\"\"\n    Sets up the testing environment for each test method.\n    \"\"\"\n    self.modes = [\n        {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n        {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n        {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n    ]\n\n    torch.manual_seed(1)\n    np.random.seed(42)\n    self.num_samples = 100\n    self.X = np.random.rand(self.num_samples, 20)\n    self.y = generate_multi_modal_data(self.num_samples, self.modes)</code></pre>"},{"location":"reference/tests/test_mdn_trainer/#tests.test_mdn_trainer.TestMDNTrainer.test_predict_with_uncertainty","title":"<code>test_predict_with_uncertainty()</code>","text":"<p>Tests the predict_with_uncertainty method of the MDNTrainer class.</p> Source code in <code>uncertaintyplayground/tests/test_mdn_trainer.py</code> <pre><code>def test_predict_with_uncertainty(self):\n    \"\"\"\n    Tests the predict_with_uncertainty method of the MDNTrainer class.\n    \"\"\"\n    trainer = MDNTrainer(self.X, self.y, num_epochs=2, lr=0.01, dense1_units=5, n_gaussians=3)\n    trainer.train()\n\n    # Generate a test instance\n    test_instance = np.random.rand(1, 20)\n    test_instance = test_instance.astype(np.float32)\n\n    pi, mu, sigma, sample = trainer.predict_with_uncertainty(test_instance)\n    # Add assertions based on the expected behavior of the predict_with_uncertainty function\n    self.assertEqual(pi.shape, (1, 3))\n    self.assertEqual(mu.shape, (1, 3))\n    self.assertEqual(sigma.shape, (1, 3))\n    self.assertIsInstance(sample, np.ndarray)</code></pre>"},{"location":"reference/tests/test_mdn_trainer/#tests.test_mdn_trainer.TestMDNTrainer.test_train","title":"<code>test_train()</code>","text":"<p>Tests the train method of the MDNTrainer class.</p> Source code in <code>uncertaintyplayground/tests/test_mdn_trainer.py</code> <pre><code>def test_train(self):\n    \"\"\"\n    Tests the train method of the MDNTrainer class.\n    \"\"\"\n    trainer = MDNTrainer(self.X, self.y, num_epochs=3, lr=0.01, dense1_units=5, n_gaussians=3, dtype = torch.float32)\n    trainer.train()</code></pre>"},{"location":"reference/tests/test_svgp_model/","title":"test_svgp_model","text":""},{"location":"reference/tests/test_svgp_model/#tests.test_svgp_model.TestSVGP","title":"<code>TestSVGP</code>","text":"<p>         Bases: <code>unittest.TestCase</code></p> <p>Unit tests for SVGP class.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_model.py</code> <pre><code>class TestSVGP(unittest.TestCase):\n    \"\"\"\n    Unit tests for SVGP class.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Test fixture setup method.\n        \"\"\"\n        self.inducing_points = torch.rand((10, 2))\n        self.dtype = torch.float32\n        self.svgp = SVGP(self.inducing_points, self.dtype)\n\n    def test_init(self):\n        \"\"\"\n        Test case for SVGP initialization.\n        \"\"\"\n        self.assertIsInstance(self.svgp, SVGP)\n        self.assertEqual(self.svgp.mean_module.constant.item(), 0.)\n\n        # Expect default initial lengthscale to be ~0.693\n        expected_lengthscale = torch.log(torch.tensor(2.0)).item()\n        self.assertAlmostEqual(self.svgp.covar_module.base_kernel.lengthscale.item(), expected_lengthscale, places=5)\n\n    def test_forward(self):\n        \"\"\"\n        Test case for forward method in SVGP.\n        \"\"\"\n        x = torch.rand((5, 2))\n        output = self.svgp.forward(x)\n        self.assertIsInstance(output, gpytorch.distributions.MultivariateNormal)\n        self.assertEqual(output.loc.shape, torch.Size([x.shape[0]]))</code></pre>"},{"location":"reference/tests/test_svgp_model/#tests.test_svgp_model.TestSVGP.setUp","title":"<code>setUp()</code>","text":"<p>Test fixture setup method.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_model.py</code> <pre><code>def setUp(self):\n    \"\"\"\n    Test fixture setup method.\n    \"\"\"\n    self.inducing_points = torch.rand((10, 2))\n    self.dtype = torch.float32\n    self.svgp = SVGP(self.inducing_points, self.dtype)</code></pre>"},{"location":"reference/tests/test_svgp_model/#tests.test_svgp_model.TestSVGP.test_forward","title":"<code>test_forward()</code>","text":"<p>Test case for forward method in SVGP.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_model.py</code> <pre><code>def test_forward(self):\n    \"\"\"\n    Test case for forward method in SVGP.\n    \"\"\"\n    x = torch.rand((5, 2))\n    output = self.svgp.forward(x)\n    self.assertIsInstance(output, gpytorch.distributions.MultivariateNormal)\n    self.assertEqual(output.loc.shape, torch.Size([x.shape[0]]))</code></pre>"},{"location":"reference/tests/test_svgp_model/#tests.test_svgp_model.TestSVGP.test_init","title":"<code>test_init()</code>","text":"<p>Test case for SVGP initialization.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_model.py</code> <pre><code>def test_init(self):\n    \"\"\"\n    Test case for SVGP initialization.\n    \"\"\"\n    self.assertIsInstance(self.svgp, SVGP)\n    self.assertEqual(self.svgp.mean_module.constant.item(), 0.)\n\n    # Expect default initial lengthscale to be ~0.693\n    expected_lengthscale = torch.log(torch.tensor(2.0)).item()\n    self.assertAlmostEqual(self.svgp.covar_module.base_kernel.lengthscale.item(), expected_lengthscale, places=5)</code></pre>"},{"location":"reference/tests/test_svgp_predplot/","title":"test_svgp_predplot","text":""},{"location":"reference/tests/test_svgp_predplot/#tests.test_svgp_predplot.TestSVGPRPlots","title":"<code>TestSVGPRPlots</code>","text":"<p>         Bases: <code>unittest.TestCase</code></p> <p>This test class provides unit tests for the compare_distributions_svgpr and plot_results_grid functions for SVGPR model.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_predplot.py</code> <pre><code>class TestSVGPRPlots(unittest.TestCase):\n    \"\"\"\n    This test class provides unit tests for the compare_distributions_svgpr and plot_results_grid functions for SVGPR model.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Sets up the testing environment for each test method.\n        \"\"\"\n        # Assuming data is generated in a similar way as in the MDN case\n        self.modes = [\n            {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n            {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n            {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n        ]\n\n        torch.manual_seed(1)\n        np.random.seed(42)\n        self.num_samples = 1000\n        self.X_test = np.random.rand(self.num_samples, 20)\n        self.Y_test = generate_multi_modal_data(self.num_samples, self.modes)\n\n        self.svgpr_trainer = SparseGPTrainer(self.X_test, self.Y_test, num_epochs=5, lr=0.01)\n        self.svgpr_trainer.train()\n\n    def test_compare_distributions_svgpr(self):\n        \"\"\"\n        Tests the compare_distributions function for the SVGPR model.\n        \"\"\"\n        index_instance = 900\n\n        with DisablePlotDisplay():\n            compare_distributions_svgpr(self.svgpr_trainer, x_instance = self.X_test[index_instance, :], y_actual=self.Y_test[index_instance])\n\n    def test_plot_results_grid_svgpr(self):\n        \"\"\"\n        Tests the plot_results_grid function with SVGPR model.\n        \"\"\"\n        indices = [900, 100]  # Example indices\n\n        # Testing with actual y values\n        with DisablePlotDisplay():\n            plot_results_grid(self.svgpr_trainer, compare_distributions_svgpr, self.X_test, indices, self.Y_test)\n\n        # Testing without actual y values\n        with DisablePlotDisplay():\n            plot_results_grid(self.svgpr_trainer, compare_distributions_svgpr, self.X_test, indices, None)</code></pre>"},{"location":"reference/tests/test_svgp_predplot/#tests.test_svgp_predplot.TestSVGPRPlots.setUp","title":"<code>setUp()</code>","text":"<p>Sets up the testing environment for each test method.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_predplot.py</code> <pre><code>def setUp(self):\n    \"\"\"\n    Sets up the testing environment for each test method.\n    \"\"\"\n    # Assuming data is generated in a similar way as in the MDN case\n    self.modes = [\n        {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n        {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n        {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n    ]\n\n    torch.manual_seed(1)\n    np.random.seed(42)\n    self.num_samples = 1000\n    self.X_test = np.random.rand(self.num_samples, 20)\n    self.Y_test = generate_multi_modal_data(self.num_samples, self.modes)\n\n    self.svgpr_trainer = SparseGPTrainer(self.X_test, self.Y_test, num_epochs=5, lr=0.01)\n    self.svgpr_trainer.train()</code></pre>"},{"location":"reference/tests/test_svgp_predplot/#tests.test_svgp_predplot.TestSVGPRPlots.test_compare_distributions_svgpr","title":"<code>test_compare_distributions_svgpr()</code>","text":"<p>Tests the compare_distributions function for the SVGPR model.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_predplot.py</code> <pre><code>def test_compare_distributions_svgpr(self):\n    \"\"\"\n    Tests the compare_distributions function for the SVGPR model.\n    \"\"\"\n    index_instance = 900\n\n    with DisablePlotDisplay():\n        compare_distributions_svgpr(self.svgpr_trainer, x_instance = self.X_test[index_instance, :], y_actual=self.Y_test[index_instance])</code></pre>"},{"location":"reference/tests/test_svgp_predplot/#tests.test_svgp_predplot.TestSVGPRPlots.test_plot_results_grid_svgpr","title":"<code>test_plot_results_grid_svgpr()</code>","text":"<p>Tests the plot_results_grid function with SVGPR model.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_predplot.py</code> <pre><code>def test_plot_results_grid_svgpr(self):\n    \"\"\"\n    Tests the plot_results_grid function with SVGPR model.\n    \"\"\"\n    indices = [900, 100]  # Example indices\n\n    # Testing with actual y values\n    with DisablePlotDisplay():\n        plot_results_grid(self.svgpr_trainer, compare_distributions_svgpr, self.X_test, indices, self.Y_test)\n\n    # Testing without actual y values\n    with DisablePlotDisplay():\n        plot_results_grid(self.svgpr_trainer, compare_distributions_svgpr, self.X_test, indices, None)</code></pre>"},{"location":"reference/tests/test_svgp_trainer/","title":"test_svgp_trainer","text":""},{"location":"reference/tests/test_svgp_trainer/#tests.test_svgp_trainer.TestSparseGPTrainer","title":"<code>TestSparseGPTrainer</code>","text":"<p>         Bases: <code>unittest.TestCase</code></p> <p>Unit test suite for the SparseGPTrainer class.</p> <p>This class contains a series of methods to test the functionalities of SparseGPTrainer, including training and making predictions with uncertainty.</p> <p>Attributes:</p> Name Type Description <code>X</code> <code>np.array</code> <p>An array of feature vectors for the training data.</p> <code>y</code> <code>np.array</code> <p>An array of target values for the training data.</p> <code>trainer</code> <code>SparseGPTrainer</code> <p>The SparseGPTrainer instance to test.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_trainer.py</code> <pre><code>class TestSparseGPTrainer(unittest.TestCase):\n    \"\"\"\n    Unit test suite for the SparseGPTrainer class.\n\n    This class contains a series of methods to test the functionalities of SparseGPTrainer, including training and making predictions with uncertainty.\n\n    Attributes:\n        X (np.array): An array of feature vectors for the training data.\n        y (np.array): An array of target values for the training data.\n        trainer (SparseGPTrainer): The SparseGPTrainer instance to test.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Setup function that runs before each test method.\n\n        This method generates random data for the tests and initializes an instance of SparseGPTrainer.\n        \"\"\"\n        self.X = np.random.rand(100, 20)\n        self.y = np.random.rand(100)\n        self.trainer = SparseGPTrainer(self.X, self.y, num_inducing_points=20, num_epochs=10, batch_size=20, lr=0.2, patience=3)\n\n    def test_train(self):\n        \"\"\"\n        Tests the train method of the SparseGPTrainer class.\n\n        This test trains the model and checks that it has non-zero parameters afterward, verifying that training has indeed happened.\n        \"\"\"\n        self.trainer.train()\n        self.assertTrue(any(p.detach().numpy().any() for p in self.trainer.model.parameters()))\n\n    def test_predict_with_uncertainty(self):\n        \"\"\"\n        Tests the predict_with_uncertainty method of the SparseGPTrainer class.\n\n        This test trains the model and then makes a prediction with uncertainty. It checks that the predictions and uncertainties have the correct shape.\n        \"\"\"\n        self.trainer.train()\n        y_pred, y_var = self.trainer.predict_with_uncertainty(self.X)\n        self.assertEqual(y_pred.shape, (self.X.shape[0],))\n        self.assertEqual(y_var.shape, (self.X.shape[0],))</code></pre>"},{"location":"reference/tests/test_svgp_trainer/#tests.test_svgp_trainer.TestSparseGPTrainer.setUp","title":"<code>setUp()</code>","text":"<p>Setup function that runs before each test method.</p> <p>This method generates random data for the tests and initializes an instance of SparseGPTrainer.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_trainer.py</code> <pre><code>def setUp(self):\n    \"\"\"\n    Setup function that runs before each test method.\n\n    This method generates random data for the tests and initializes an instance of SparseGPTrainer.\n    \"\"\"\n    self.X = np.random.rand(100, 20)\n    self.y = np.random.rand(100)\n    self.trainer = SparseGPTrainer(self.X, self.y, num_inducing_points=20, num_epochs=10, batch_size=20, lr=0.2, patience=3)</code></pre>"},{"location":"reference/tests/test_svgp_trainer/#tests.test_svgp_trainer.TestSparseGPTrainer.test_predict_with_uncertainty","title":"<code>test_predict_with_uncertainty()</code>","text":"<p>Tests the predict_with_uncertainty method of the SparseGPTrainer class.</p> <p>This test trains the model and then makes a prediction with uncertainty. It checks that the predictions and uncertainties have the correct shape.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_trainer.py</code> <pre><code>def test_predict_with_uncertainty(self):\n    \"\"\"\n    Tests the predict_with_uncertainty method of the SparseGPTrainer class.\n\n    This test trains the model and then makes a prediction with uncertainty. It checks that the predictions and uncertainties have the correct shape.\n    \"\"\"\n    self.trainer.train()\n    y_pred, y_var = self.trainer.predict_with_uncertainty(self.X)\n    self.assertEqual(y_pred.shape, (self.X.shape[0],))\n    self.assertEqual(y_var.shape, (self.X.shape[0],))</code></pre>"},{"location":"reference/tests/test_svgp_trainer/#tests.test_svgp_trainer.TestSparseGPTrainer.test_train","title":"<code>test_train()</code>","text":"<p>Tests the train method of the SparseGPTrainer class.</p> <p>This test trains the model and checks that it has non-zero parameters afterward, verifying that training has indeed happened.</p> Source code in <code>uncertaintyplayground/tests/test_svgp_trainer.py</code> <pre><code>def test_train(self):\n    \"\"\"\n    Tests the train method of the SparseGPTrainer class.\n\n    This test trains the model and checks that it has non-zero parameters afterward, verifying that training has indeed happened.\n    \"\"\"\n    self.trainer.train()\n    self.assertTrue(any(p.detach().numpy().any() for p in self.trainer.model.parameters()))</code></pre>"},{"location":"reference/trainers/base_trainer/","title":"base_trainer","text":""},{"location":"reference/trainers/base_trainer/#trainers.base_trainer.BaseTrainer","title":"<code>BaseTrainer</code>","text":"<p>Base trainer class for model training.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or torch.Tensor</code> <p>Input data of shape (num_samples, num_features).</p> required <code>y</code> <code>np.ndarray or torch.Tensor</code> <p>Target data of shape (num_samples,).</p> required <code>sample_weights</code> <code>np.ndarray or torch.Tensor</code> <p>Optional sample weights of shape (num_samples,).</p> <code>None</code> <code>test_size</code> <code>float</code> <p>The proportion of the data to include in the validation set.</p> <code>0.2</code> <code>random_state</code> <code>int</code> <p>The seed used by the random number generator.</p> <code>42</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>50</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>256</code> <code>optimizer_fn_name</code> <code>str</code> <p>Name of the optimizer function from the <code>torch.optim</code> module.</p> <code>'Adam'</code> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.01</code> <code>use_scheduler</code> <code>bool</code> <p>Whether to use a learning rate scheduler (<code>not yet fully supported</code>).</p> <code>False</code> <code>patience</code> <code>int</code> <p>Number of consecutive epochs with no improvement after which training will be stopped.</p> <code>10</code> <code>dtype</code> <code>torch.dtype</code> <p>Data type to use for the tensors.</p> <code>torch.float32</code> <p>Attributes:</p> Name Type Description <code>X</code> <code>torch.Tensor</code> <p>Input data tensor.</p> <code>y</code> <code>torch.Tensor</code> <p>Target data tensor.</p> <code>sample_weights</code> <code>torch.Tensor</code> <p>Sample weights tensor.</p> <code>test_size</code> <code>float</code> <p>Proportion of data to include in the validation set.</p> <code>random_state</code> <code>int</code> <p>Seed used by the random number generator.</p> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>optimizer_fn_name</code> <code>str</code> <p>Name of the optimizer function.</p> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>patience</code> <code>int</code> <p>Number of consecutive epochs with no improvement after which training will be stopped.</p> <code>dtype</code> <code>torch.dtype</code> <p>Data type of the tensors.</p> <code>device</code> <code>torch.device</code> <p>Device (GPU if available, otherwise CPU).</p> <code>train_loader</code> <code>torch.utils.data.DataLoader</code> <p>DataLoader for training data.</p> Source code in <code>uncertaintyplayground/trainers/base_trainer.py</code> <pre><code>class BaseTrainer:\n    \"\"\"\n    Base trainer class for model training.\n\n    Args:\n        X (np.ndarray or torch.Tensor): Input data of shape (num_samples, num_features).\n        y (np.ndarray or torch.Tensor): Target data of shape (num_samples,).\n        sample_weights (np.ndarray or torch.Tensor): Optional sample weights of shape (num_samples,).\n        test_size (float): The proportion of the data to include in the validation set.\n        random_state (int): The seed used by the random number generator.\n        num_epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_fn_name (str): Name of the optimizer function from the `torch.optim` module.\n        lr (float): Learning rate for the optimizer.\n        use_scheduler (bool): Whether to use a learning rate scheduler (`not yet fully supported`).\n        patience (int): Number of consecutive epochs with no improvement after which training will be stopped.\n        dtype (torch.dtype): Data type to use for the tensors.\n\n    Attributes:\n        X (torch.Tensor): Input data tensor.\n        y (torch.Tensor): Target data tensor.\n        sample_weights (torch.Tensor): Sample weights tensor.\n        test_size (float): Proportion of data to include in the validation set.\n        random_state (int): Seed used by the random number generator.\n        num_epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_fn_name (str): Name of the optimizer function.\n        lr (float): Learning rate for the optimizer.\n        patience (int): Number of consecutive epochs with no improvement after which training will be stopped.\n        dtype (torch.dtype): Data type of the tensors.\n        device (torch.device): Device (GPU if available, otherwise CPU).\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n    \"\"\"\n\n    def __init__(\n            self,\n            X,\n            y,\n            sample_weights=None,\n            test_size=0.2,\n            random_state=42,\n            num_epochs=50,\n            batch_size=256,\n            optimizer_fn_name=\"Adam\",\n            lr=0.01,\n            use_scheduler=False,\n            patience=10,\n            dtype=torch.float32\n    ):\n        self.X = X\n        self.y = y\n        self.sample_weights = sample_weights\n        self.test_size = test_size\n        self.random_state = random_state\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.optimizer_fn_name = optimizer_fn_name\n        self.lr = lr\n        self.patience = patience\n        self.dtype = dtype\n        self.use_scheduler = use_scheduler\n\n        # Setting for early stopping\n        self.best_epoch = -1\n        self.best_val_mse = np.inf\n\n        # Choose device (GPU if available, otherwise CPU)\n        self.device = torch.device(\n            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # Convert input tensors to the correct type\n        self.prepare_inputs()\n\n        # Split data into training and validation sets\n        self.split_data()\n\n        # Create DataLoader for training data\n        self.prepare_dataloader()\n\n    def prepare_inputs(self):\n        \"\"\"\n        Convert input data to the correct type and format.\n        \"\"\"\n        # Convert X to a tensor if it's a numpy array\n        if isinstance(self.X, np.ndarray):\n            self.X = torch.from_numpy(self.X).to(self.dtype)\n\n        # Convert y to a tensor if it's a list or numpy array\n        if isinstance(self.y, (list, np.ndarray)):\n            self.y = torch.tensor(self.y, dtype=self.dtype)\n\n        # Check if sample_weights is a tensor, numpy array, or list\n        if self.sample_weights is not None:\n            if isinstance(self.sample_weights, (np.ndarray, list)):\n                self.sample_weights = torch.tensor(\n                    self.sample_weights, dtype=self.dtype)\n\n    def split_data(self, test_size=0.2):\n        \"\"\"\n        Split the data into training and validation sets.\n\n        Args:\n            test_size (float): Proportion of data to include in the validation set.\n        \"\"\"\n        if self.sample_weights is None:\n            self.sample_weights = torch.ones(self.X.shape[0], dtype=self.dtype)\n\n        self.X_train, self.X_val, self.y_train, self.y_val, self.sample_weights_train, self.sample_weights_val = \\\n            train_test_split(self.X, self.y, self.sample_weights,\n                             test_size=test_size, random_state=self.random_state)\n\n    def custom_lr_scheduler(self, epoch):\n        \"\"\"\n        Custom learning rate scheduler function.\n\n        Args:\n            epoch (int): Current epoch.\n\n        Returns:\n            float: Learning rate for the epoch.\n        \"\"\"\n        if epoch &lt; 3:\n            return 1 - 0.1 * epoch / self.lr\n        else:\n            return 0.2 / self.lr\n\n    def prepare_dataloader(self):\n        \"\"\"\n        Prepare the DataLoader for training data.\n        \"\"\"\n        # Use all available CPU cores or default to 1 if not detected\n        num_workers = os.cpu_count() - 1 or 1\n        train_dataset = TensorDataset(\n            self.X_train, self.y_train, self.sample_weights_train)\n        self.train_loader = DataLoader(\n            train_dataset, batch_size=self.batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n        )</code></pre>"},{"location":"reference/trainers/base_trainer/#trainers.base_trainer.BaseTrainer.custom_lr_scheduler","title":"<code>custom_lr_scheduler(epoch)</code>","text":"<p>Custom learning rate scheduler function.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Current epoch.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Learning rate for the epoch.</p> Source code in <code>uncertaintyplayground/trainers/base_trainer.py</code> <pre><code>def custom_lr_scheduler(self, epoch):\n    \"\"\"\n    Custom learning rate scheduler function.\n\n    Args:\n        epoch (int): Current epoch.\n\n    Returns:\n        float: Learning rate for the epoch.\n    \"\"\"\n    if epoch &lt; 3:\n        return 1 - 0.1 * epoch / self.lr\n    else:\n        return 0.2 / self.lr</code></pre>"},{"location":"reference/trainers/base_trainer/#trainers.base_trainer.BaseTrainer.prepare_dataloader","title":"<code>prepare_dataloader()</code>","text":"<p>Prepare the DataLoader for training data.</p> Source code in <code>uncertaintyplayground/trainers/base_trainer.py</code> <pre><code>def prepare_dataloader(self):\n    \"\"\"\n    Prepare the DataLoader for training data.\n    \"\"\"\n    # Use all available CPU cores or default to 1 if not detected\n    num_workers = os.cpu_count() - 1 or 1\n    train_dataset = TensorDataset(\n        self.X_train, self.y_train, self.sample_weights_train)\n    self.train_loader = DataLoader(\n        train_dataset, batch_size=self.batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n    )</code></pre>"},{"location":"reference/trainers/base_trainer/#trainers.base_trainer.BaseTrainer.prepare_inputs","title":"<code>prepare_inputs()</code>","text":"<p>Convert input data to the correct type and format.</p> Source code in <code>uncertaintyplayground/trainers/base_trainer.py</code> <pre><code>def prepare_inputs(self):\n    \"\"\"\n    Convert input data to the correct type and format.\n    \"\"\"\n    # Convert X to a tensor if it's a numpy array\n    if isinstance(self.X, np.ndarray):\n        self.X = torch.from_numpy(self.X).to(self.dtype)\n\n    # Convert y to a tensor if it's a list or numpy array\n    if isinstance(self.y, (list, np.ndarray)):\n        self.y = torch.tensor(self.y, dtype=self.dtype)\n\n    # Check if sample_weights is a tensor, numpy array, or list\n    if self.sample_weights is not None:\n        if isinstance(self.sample_weights, (np.ndarray, list)):\n            self.sample_weights = torch.tensor(\n                self.sample_weights, dtype=self.dtype)</code></pre>"},{"location":"reference/trainers/base_trainer/#trainers.base_trainer.BaseTrainer.split_data","title":"<code>split_data(test_size=0.2)</code>","text":"<p>Split the data into training and validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float</code> <p>Proportion of data to include in the validation set.</p> <code>0.2</code> Source code in <code>uncertaintyplayground/trainers/base_trainer.py</code> <pre><code>def split_data(self, test_size=0.2):\n    \"\"\"\n    Split the data into training and validation sets.\n\n    Args:\n        test_size (float): Proportion of data to include in the validation set.\n    \"\"\"\n    if self.sample_weights is None:\n        self.sample_weights = torch.ones(self.X.shape[0], dtype=self.dtype)\n\n    self.X_train, self.X_val, self.y_train, self.y_val, self.sample_weights_train, self.sample_weights_val = \\\n        train_test_split(self.X, self.y, self.sample_weights,\n                         test_size=test_size, random_state=self.random_state)</code></pre>"},{"location":"reference/trainers/mdn_trainer/","title":"mdn_trainer","text":""},{"location":"reference/trainers/mdn_trainer/#trainers.mdn_trainer.MDNTrainer","title":"<code>MDNTrainer</code>","text":"<p>         Bases: <code>BaseTrainer</code></p> <p>Trainer for the Mixed Density Network (MDN) model.</p> <p>This class handles the training process for the MDN model.</p> <p>Parameters:</p> Name Type Description Default <code>dense1_units</code> <code>int</code> <p>Number of hidden units in first layer of the neural network.</p> <code>20</code> <code>n_gaussians</code> <code>int</code> <p>Number of Gaussian components in the mixture.</p> <code>5</code> <code>**kwargs</code> <p>Additional arguments passed to the BaseTrainer.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>n_gaussians</code> <code>int</code> <p>Number of Gaussian components in the mixture.</p> <code>model</code> <code>MDN</code> <p>The MDN model.</p> <code>optimizer</code> <code>torch.optim.Optimizer</code> <p>The optimizer for model training.</p> Source code in <code>uncertaintyplayground/trainers/mdn_trainer.py</code> <pre><code>class MDNTrainer(BaseTrainer):\n    \"\"\"\n    Trainer for the Mixed Density Network (MDN) model.\n\n    This class handles the training process for the MDN model.\n\n    Args:\n        dense1_units (int): Number of hidden units in first layer of the neural network.\n        n_gaussians (int): Number of Gaussian components in the mixture.\n        **kwargs: Additional arguments passed to the BaseTrainer.\n\n    Attributes:\n        n_gaussians (int): Number of Gaussian components in the mixture.\n        model (MDN): The MDN model.\n        optimizer (torch.optim.Optimizer): The optimizer for model training.\n    \"\"\"\n\n    def __init__(self, *args, dense1_units=20, n_gaussians=5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.n_gaussians = n_gaussians\n\n        self.model = MDN(input_dim=self.X.shape[1], n_gaussians=self.n_gaussians, dense1_units = dense1_units).to(self.device)\n        if self.dtype == torch.float64:\n            self.model = self.model.double()  # Convert model parameters to float64\n        optimizer_fn = getattr(torch.optim, self.optimizer_fn_name)\n        self.optimizer = optimizer_fn(self.model.parameters(), lr=self.lr)\n\n    def train(self):\n        \"\"\"\n        Train the MDN model.\n        \"\"\"\n        self.model.train()\n        early_stopping = EarlyStopping(patience=self.patience, compare_fn=lambda x, y: x &lt; y)\n\n        for epoch in range(self.num_epochs):\n            for X_batch, y_batch, weights_batch in self.train_loader:\n                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n\n                self.optimizer.zero_grad()\n\n                pi, mu, sigma = self.model(X_batch)\n                loss = mdn_loss(y_batch, mu, sigma, pi)\n\n                loss.backward()\n                self.optimizer.step()\n\n            self.model.eval()\n            with torch.no_grad():\n                pi, mu, sigma = self.model(self.X_val.to(self.device))\n                val_loss = mdn_loss(self.y_val.to(self.device), mu, sigma, pi)\n\n            self.model.train()\n\n            print(\n                f\"Epoch {epoch + 1}/{self.num_epochs}, Training Loss: {loss.item():.3f}, \"\n                f\"Validation Loss: {val_loss.item():.3f}\"\n            )\n\n            should_stop = early_stopping(val_loss.item(), self.model)\n\n            if should_stop:\n                print(f\"Early stopping after {epoch + 1} epochs\")\n                break\n\n        if early_stopping.best_model_state is not None:\n            self.model.load_state_dict(early_stopping.best_model_state)\n            self.model.eval()\n\n    def predict_with_uncertainty(self, X):\n        \"\"\"\n        Predict the output distribution given input data.\n\n        Args:\n            X (np.ndarray or torch.Tensor): Input data of shape (num_samples, num_features).\n\n        Returns:\n            tuple: A tuple containing the predicted mixture weights, means, standard deviations, and samples.\n        \"\"\"\n        self.model.eval()\n\n        # Convert numpy array to PyTorch tensor if necessary\n        if isinstance(X, np.ndarray):\n            X = torch.from_numpy(X).to(self.device)\n\n        # Check if X is a single instance and add an extra dimension if necessary\n        if X.ndim == 1:\n            X = torch.unsqueeze(X, 0)\n\n        with torch.no_grad():\n            pi, mu, sigma = self.model(X)\n            sample = self.model.sample(X, num_samples=1000)\n\n        return pi.cpu().numpy(), mu.cpu().numpy(), sigma.cpu().numpy(), sample.cpu().numpy()</code></pre>"},{"location":"reference/trainers/mdn_trainer/#trainers.mdn_trainer.MDNTrainer.predict_with_uncertainty","title":"<code>predict_with_uncertainty(X)</code>","text":"<p>Predict the output distribution given input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or torch.Tensor</code> <p>Input data of shape (num_samples, num_features).</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the predicted mixture weights, means, standard deviations, and samples.</p> Source code in <code>uncertaintyplayground/trainers/mdn_trainer.py</code> <pre><code>def predict_with_uncertainty(self, X):\n    \"\"\"\n    Predict the output distribution given input data.\n\n    Args:\n        X (np.ndarray or torch.Tensor): Input data of shape (num_samples, num_features).\n\n    Returns:\n        tuple: A tuple containing the predicted mixture weights, means, standard deviations, and samples.\n    \"\"\"\n    self.model.eval()\n\n    # Convert numpy array to PyTorch tensor if necessary\n    if isinstance(X, np.ndarray):\n        X = torch.from_numpy(X).to(self.device)\n\n    # Check if X is a single instance and add an extra dimension if necessary\n    if X.ndim == 1:\n        X = torch.unsqueeze(X, 0)\n\n    with torch.no_grad():\n        pi, mu, sigma = self.model(X)\n        sample = self.model.sample(X, num_samples=1000)\n\n    return pi.cpu().numpy(), mu.cpu().numpy(), sigma.cpu().numpy(), sample.cpu().numpy()</code></pre>"},{"location":"reference/trainers/mdn_trainer/#trainers.mdn_trainer.MDNTrainer.train","title":"<code>train()</code>","text":"<p>Train the MDN model.</p> Source code in <code>uncertaintyplayground/trainers/mdn_trainer.py</code> <pre><code>def train(self):\n    \"\"\"\n    Train the MDN model.\n    \"\"\"\n    self.model.train()\n    early_stopping = EarlyStopping(patience=self.patience, compare_fn=lambda x, y: x &lt; y)\n\n    for epoch in range(self.num_epochs):\n        for X_batch, y_batch, weights_batch in self.train_loader:\n            X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n\n            self.optimizer.zero_grad()\n\n            pi, mu, sigma = self.model(X_batch)\n            loss = mdn_loss(y_batch, mu, sigma, pi)\n\n            loss.backward()\n            self.optimizer.step()\n\n        self.model.eval()\n        with torch.no_grad():\n            pi, mu, sigma = self.model(self.X_val.to(self.device))\n            val_loss = mdn_loss(self.y_val.to(self.device), mu, sigma, pi)\n\n        self.model.train()\n\n        print(\n            f\"Epoch {epoch + 1}/{self.num_epochs}, Training Loss: {loss.item():.3f}, \"\n            f\"Validation Loss: {val_loss.item():.3f}\"\n        )\n\n        should_stop = early_stopping(val_loss.item(), self.model)\n\n        if should_stop:\n            print(f\"Early stopping after {epoch + 1} epochs\")\n            break\n\n    if early_stopping.best_model_state is not None:\n        self.model.load_state_dict(early_stopping.best_model_state)\n        self.model.eval()</code></pre>"},{"location":"reference/trainers/svgp_trainer/","title":"svgp_trainer","text":""},{"location":"reference/trainers/svgp_trainer/#trainers.svgp_trainer.SparseGPTrainer","title":"<code>SparseGPTrainer</code>","text":"<p>         Bases: <code>BaseTrainer</code></p> <p>Trains an SVGP model using specified parameters and early stopping.</p> <p>Attributes:</p> Name Type Description <code>num_inducing_points</code> <code>int</code> <p>Number of inducing points for the SVGP.</p> <code>model</code> <code>SVGP</code> <p>The Stochastic Variational Gaussian Process model.</p> <code>likelihood</code> <code>gpytorch.likelihoods.GaussianLikelihood</code> <p>The likelihood of the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like</code> <p>The input features.</p> required <code>y</code> <code>array-like</code> <p>The target outputs.</p> required <code>num_inducing_points</code> <code>int</code> <p>Number of inducing points to use in the SVGP model.</p> <code>100</code> <code>sample_weights</code> <code>array-like</code> <p>Sample weights for each data point. Defaults to None.</p> required <code>test_size</code> <code>float</code> <p>Fraction of the dataset to be used as test data. Defaults to 0.2.</p> required <code>random_state</code> <code>int</code> <p>Random seed for reproducible results. Defaults to 42.</p> required <code>num_epochs</code> <code>int</code> <p>Maximum number of training epochs. Defaults to 50.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 256.</p> required <code>optimizer_fn_name</code> <code>str</code> <p>Name of the optimizer to use. Defaults to \"Adam\".</p> required <code>lr</code> <code>float</code> <p>Learning rate for the optimizer. Defaults to 0.01.</p> required <code>use_scheduler</code> <code>bool</code> <p>Whether to use a learning rate scheduler. Defaults to False.</p> required <code>patience</code> <code>int</code> <p>Number of epochs with no improvement before stopping training. Defaults to 10.</p> required <code>dtype</code> <code>torch.dtype</code> <p>The dtype to use for input tensors. Defaults to torch.float32.</p> required Source code in <code>uncertaintyplayground/trainers/svgp_trainer.py</code> <pre><code>class SparseGPTrainer(BaseTrainer):\n    \"\"\"\n    Trains an SVGP model using specified parameters and early stopping.\n\n    Attributes:\n        num_inducing_points (int): Number of inducing points for the SVGP.\n        model (SVGP): The Stochastic Variational Gaussian Process model.\n        likelihood (gpytorch.likelihoods.GaussianLikelihood): The likelihood of the model.\n\n    Args:\n        X (array-like): The input features.\n        y (array-like): The target outputs.\n        num_inducing_points (int): Number of inducing points to use in the SVGP model.\n        sample_weights (array-like, optional): Sample weights for each data point. Defaults to None.\n        test_size (float, optional): Fraction of the dataset to be used as test data. Defaults to 0.2.\n        random_state (int, optional): Random seed for reproducible results. Defaults to 42.\n        num_epochs (int, optional): Maximum number of training epochs. Defaults to 50.\n        batch_size (int, optional): Batch size for training. Defaults to 256.\n        optimizer_fn_name (str, optional): Name of the optimizer to use. Defaults to \"Adam\".\n        lr (float, optional): Learning rate for the optimizer. Defaults to 0.01.\n        use_scheduler (bool, optional): Whether to use a learning rate scheduler. Defaults to False.\n        patience (int, optional): Number of epochs with no improvement before stopping training. Defaults to 10.\n        dtype (torch.dtype, optional): The dtype to use for input tensors. Defaults to torch.float32.\n    \"\"\"\n    def __init__(self, *args, num_inducing_points=100, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_inducing_points = num_inducing_points\n\n        # Initialize the model with inducing points\n        inducing_points = self.X_train[:num_inducing_points, :]\n        self.model = SVGP(inducing_points, dtype=self.dtype).to(\n            self.device, dtype=self.dtype)\n        self.likelihood = gpytorch.likelihoods.GaussianLikelihood(\n            dtype=self.dtype).to(self.device, dtype=self.dtype)\n\n    def train(self):\n        # set the seed\n        torch.manual_seed(self.random_state)\n\n        # define the optimizer &amp; loss function (dynamically)\n        optimizer_fn = getattr(torch.optim, self.optimizer_fn_name)\n        optimizer = optimizer_fn(self.model.parameters(), lr=self.lr)\n\n        # can use either one of the schuedlers\n        # define the learning rate scheduler if use_scheduler is True\n        # if self.use_scheduler:\n        #     scheduler = torch.optim.lr_scheduler.LambdaLR(\n        #         optimizer, self.custom_lr_scheduler)\n        if self.use_scheduler:\n            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 5, epochs=50,  steps_per_epoch=len(self.train_loader))\n\n        # define the loss function\n        mll = gpytorch.mlls.VariationalELBO(\n            self.likelihood, self.model, num_data=self.X_train.shape[0])\n\n        # Early stopping parameters\n        early_stopping = EarlyStopping(\n            patience=self.patience, compare_fn=lambda x, y: x &lt; y)\n\n        # Initiate the model training mode\n        self.model.train()\n        self.likelihood.train()\n\n        for i in range(self.num_epochs):\n            for X_batch, y_batch, weights_batch in self.train_loader:\n                X_batch, y_batch, weights_batch = X_batch.to(self.device, dtype=self.dtype), y_batch.to(\n                    self.device, dtype=self.dtype), weights_batch.to(self.device, dtype=self.dtype)  # Move tensors to the chosen device\n                optimizer.zero_grad()\n\n                output = self.model(X_batch)\n                unweighted_loss = -mll(output, y_batch)\n\n                # Apply sample weights\n                weighted_loss = torch.mean(unweighted_loss * weights_batch)\n\n                weighted_loss.backward()\n\n                optimizer.step()\n\n                # the scheduler is called after the optimizer\n                if self.use_scheduler:\n                    scheduler.step()                \n\n            # if self.use_scheduler and i &gt;= 2:\n            #     scheduler.step()\n            # if self.use_scheduler:\n            #     scheduler.step()\n\n            # Compute validation metrics (MSE and R2)\n            self.model.eval()\n            self.likelihood.eval()\n            with torch.no_grad(), gpytorch.settings.fast_pred_var():\n                y_pred_val = self.likelihood(self.model(self.X_val)).mean\n\n            mse_val = mean_squared_error(\n                self.y_val.detach().numpy(), y_pred_val.detach().numpy())\n            r2_val = r2_score(self.y_val.detach().numpy(),\n                              y_pred_val.detach().numpy())\n\n            self.model.train()\n            self.likelihood.train()\n\n            print(\n                f\"Epoch {i + 1}/{self.num_epochs}, Weighted Loss: {weighted_loss.item():.3f}, Val MSE: {mse_val:.6f}, Val R2: {r2_val:.3f}\")\n\n            should_stop = early_stopping(mse_val, self.model)\n\n            if should_stop:\n                print(f\"Early stopping after {i + 1} epochs\")\n                break\n\n        if early_stopping.best_model_state is not None:\n            self.model.load_state_dict(early_stopping.best_model_state)\n            self.model.eval()\n            self.likelihood.eval()\n\n    def predict_with_uncertainty(self, X):\n        \"\"\"\n        Predicts the mean and variance of the output distribution given input tensor X.\n\n        Args:\n            X (tensor): Input tensor of shape (num_samples, num_features).\n\n        Returns:\n            tuple: A tuple of the mean and variance of the output distribution, both of shape (num_samples,).\n        \"\"\"\n        self.model.eval()\n        self.likelihood.eval()\n\n        # Convert numpy array to PyTorch tensor if necessary\n        if isinstance(X, np.ndarray):\n            X = torch.from_numpy(X).to(self.dtype)\n\n        # Check if X is a single instance and add an extra dimension if necessary\n        if X.ndim == 1:\n            X = torch.unsqueeze(X, 0)\n\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            # Get the predictive mean and variance\n            preds = self.likelihood(self.model(X))\n            mean = preds.mean.cpu().numpy()\n            variance = preds.variance.cpu().numpy()\n\n        return mean, variance</code></pre>"},{"location":"reference/trainers/svgp_trainer/#trainers.svgp_trainer.SparseGPTrainer.predict_with_uncertainty","title":"<code>predict_with_uncertainty(X)</code>","text":"<p>Predicts the mean and variance of the output distribution given input tensor X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tensor</code> <p>Input tensor of shape (num_samples, num_features).</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of the mean and variance of the output distribution, both of shape (num_samples,).</p> Source code in <code>uncertaintyplayground/trainers/svgp_trainer.py</code> <pre><code>def predict_with_uncertainty(self, X):\n    \"\"\"\n    Predicts the mean and variance of the output distribution given input tensor X.\n\n    Args:\n        X (tensor): Input tensor of shape (num_samples, num_features).\n\n    Returns:\n        tuple: A tuple of the mean and variance of the output distribution, both of shape (num_samples,).\n    \"\"\"\n    self.model.eval()\n    self.likelihood.eval()\n\n    # Convert numpy array to PyTorch tensor if necessary\n    if isinstance(X, np.ndarray):\n        X = torch.from_numpy(X).to(self.dtype)\n\n    # Check if X is a single instance and add an extra dimension if necessary\n    if X.ndim == 1:\n        X = torch.unsqueeze(X, 0)\n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        # Get the predictive mean and variance\n        preds = self.likelihood(self.model(X))\n        mean = preds.mean.cpu().numpy()\n        variance = preds.variance.cpu().numpy()\n\n    return mean, variance</code></pre>"},{"location":"reference/utils/early_stopping/","title":"early_stopping","text":""},{"location":"reference/utils/early_stopping/#utils.early_stopping.EarlyStopping","title":"<code>EarlyStopping</code>","text":"<p>Early stopping utility class for model training.</p> <p>Stops the training process when a specified performance metric does not improve for a specified number of consecutive epochs.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of consecutive epochs with no improvement after which training will be stopped.</p> <code>10</code> <code>compare_fn</code> <code>callable</code> <p>Function to compare two values of the validation metric to determine if one is better than the other.</p> <code>lambda x, y: x &lt; y</code> Source code in <code>uncertaintyplayground/utils/early_stopping.py</code> <pre><code>class EarlyStopping:\n    \"\"\"\n    Early stopping utility class for model training.\n\n    Stops the training process when a specified performance metric does not improve for a specified number of consecutive epochs.\n\n    Args:\n        patience (int): Number of consecutive epochs with no improvement after which training will be stopped.\n        compare_fn (callable): Function to compare two values of the validation metric to determine if one is better than the other.\n    \"\"\"\n\n    def __init__(self, patience=10, compare_fn=lambda x, y: x &lt; y):\n        self.patience = patience\n        self.counter = 0\n        self.best_val_metric = np.inf\n        self.best_model_state = None\n        self.compare_fn = compare_fn\n\n    def __call__(self, val_metric, model):\n        if self.compare_fn(val_metric, self.best_val_metric):\n            self.best_val_metric = val_metric\n            self.counter = 0\n            self.best_model_state = {k: v.clone() for k, v in model.state_dict().items()}\n        else:\n            self.counter += 1\n\n        if self.counter &gt;= self.patience:\n            return True\n\n        return False</code></pre>"},{"location":"reference/utils/generate_data/","title":"generate_data","text":""},{"location":"reference/utils/generate_data/#utils.generate_data.generate_multi_modal_data","title":"<code>generate_multi_modal_data(num_samples, modes)</code>","text":"<p>Generate multimodal data for the mixture density network.</p> <p>This function generates a specified number of samples for each mode. Each mode is defined by a mean, standard deviation, and weight. The weight determines the proportion of total samples that will come from this mode.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The total number of data samples to generate.</p> required <code>modes</code> <code>list of dict</code> <p>A list of dictionaries, where each dictionary represents a mode and contains the keys 'mean' (float), 'std_dev' (float), and 'weight' (float).</p> required <p>Returns:</p> Type Description <p>np.array: An array of generated data samples.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_samples is not a positive integer.</p> <code>ValueError</code> <p>If modes is not a list of dictionaries each containing 'mean', 'std_dev', and 'weight'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; modes = [\n...     {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n...     {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n...     {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n... ]\n&gt;&gt;&gt; data = generate_multi_modal_data(1000, modes)\n&gt;&gt;&gt; print(data.shape)\n(1000,)</code></pre> Source code in <code>uncertaintyplayground/utils/generate_data.py</code> <pre><code>def generate_multi_modal_data(num_samples, modes):\n    \"\"\"\n    Generate multimodal data for the mixture density network.\n\n    This function generates a specified number of samples for each mode. Each mode is defined by\n    a mean, standard deviation, and weight. The weight determines the proportion of total samples\n    that will come from this mode.\n\n    Args:\n        num_samples (int): The total number of data samples to generate.\n        modes (list of dict): A list of dictionaries, where each dictionary represents a mode and\n            contains the keys 'mean' (float), 'std_dev' (float), and 'weight' (float).\n\n    Returns:\n        np.array: An array of generated data samples.\n\n    Raises:\n        ValueError: If num_samples is not a positive integer.\n        ValueError: If modes is not a list of dictionaries each containing 'mean', 'std_dev', and 'weight'.\n\n    Examples:\n        &gt;&gt;&gt; modes = [\n        ...     {'mean': -3.0, 'std_dev': 0.5, 'weight': 0.3},\n        ...     {'mean': 0.0, 'std_dev': 1.0, 'weight': 0.4},\n        ...     {'mean': 3.0, 'std_dev': 0.7, 'weight': 0.3}\n        ... ]\n        &gt;&gt;&gt; data = generate_multi_modal_data(1000, modes)\n        &gt;&gt;&gt; print(data.shape)\n        (1000,)\n    \"\"\"\n    samples = []\n    for mode in modes:\n        mean, std_dev = mode['mean'], mode['std_dev']\n        num = int(num_samples * mode['weight'])\n        samples.extend(np.random.normal(mean, std_dev, num))\n    return np.array(samples)</code></pre>"}]}